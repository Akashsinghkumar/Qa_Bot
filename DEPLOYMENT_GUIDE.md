# üöÄ Render Deployment Guide for Q&A Bot

## üìã Prerequisites

1. **Ollama Instance**: You need a running Ollama service accessible from the internet
2. **Render Account**: Free or paid Render account
3. **Git Repository**: Your code should be in a Git repository

## üîß Environment Variables Setup

### Required Variables (Set in Render Dashboard ‚Üí Environment ‚Üí Add Variable)

| Variable | Value | Description |
|----------|-------|-------------|
| `MODEL_URL` | `https://your-ollama-domain.com/api/generate` | Your Ollama API endpoint |
| `MODEL_NAME` | `gemma:2b` | Model name to use |
| `SECRET_KEY` | `auto-generated` | Flask secret key (auto-generated by Render) |
| `RENDER` | `true` | Environment flag |
| `REQUEST_TIMEOUT` | `30` | Timeout for Ollama API calls (seconds) |
| `SPEECH_TIMEOUT` | `10` | Timeout for Google Speech Recognition (seconds) |
| `GTTS_TIMEOUT` | `30` | Timeout for Google Text-to-Speech (seconds) |

### Optional Variables

| Variable | Value | Description |
|----------|-------|-------------|
| `TESSERACT_CMD` | `/usr/bin/tesseract` | Tesseract OCR path |
| `FLASK_ENV` | `production` | Flask environment |

## üåê Ollama Setup Options

### Option 1: Self-Hosted Ollama (Recommended)
```bash
# On your VPS/Server
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull gemma:2b

# Make it accessible via nginx reverse proxy
# Example nginx config:
server {
    listen 80;
    server_name your-domain.com;
    
    location /api/ {
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### Option 2: Cloud Ollama Services
- **Ollama Cloud**: https://ollama.ai/cloud
- **RunPod**: https://runpod.io/
- **Hugging Face**: https://huggingface.co/

### Option 3: Local Development Only
For local testing, keep `MODEL_URL` as `http://localhost:11434/api/generate`

## üì± Render Deployment Steps

1. **Connect Repository**
   - Go to Render Dashboard
   - Click "New +" ‚Üí "Web Service"
   - Connect your Git repository

2. **Configure Service**
   - **Name**: `qa-bot`
   - **Environment**: `Python`
   - **Build Command**: 
     ```bash
     pip install --upgrade pip
     pip install -r requirements.txt
     ```
   - **Start Command**: 
     ```bash
     gunicorn app:app --bind 0.0.0.0:$PORT --timeout 120 --workers 1 --preload
     ```

3. **Set Environment Variables**
   - Add all required variables from the table above
   - Make sure `MODEL_URL` points to your accessible Ollama instance

4. **Deploy**
   - Click "Create Web Service"
   - Wait for build to complete
   - Check logs for any errors

## üîç Troubleshooting

### Common Issues

1. **Connection Timeout**
   ```
   ‚ùå Connection error - Cannot reach Ollama service
   ```
   - Check if `MODEL_URL` is correct
   - Verify Ollama service is running and accessible
   - Check firewall/security group settings

2. **Build Failures**
   - Check Python version compatibility
   - Verify all dependencies in `requirements.txt`
   - Check build logs for specific errors

3. **Runtime Errors**
   - Check application logs in Render Dashboard
   - Verify environment variables are set correctly
   - Test health endpoint: `/health`

### Health Check Endpoint

Visit `/health` to check:
- Application status
- Ollama service connectivity
- Configuration validation
- Environment information

## üìä Monitoring

### Render Dashboard
- **Logs**: Real-time application logs
- **Metrics**: Response times, error rates
- **Deployments**: Build and deployment history

### Application Logs
- All API calls are logged with timestamps
- Error details are captured for debugging
- Performance metrics are tracked

## üîí Security Considerations

1. **API Keys**: Never commit API keys to Git
2. **Secret Key**: Use Render's auto-generated `SECRET_KEY`
3. **HTTPS**: Render provides SSL certificates automatically
4. **Rate Limiting**: Consider adding rate limiting for production

## üöÄ Performance Optimization

1. **Worker Configuration**: Single worker with preload for memory efficiency
2. **Timeout Settings**: Optimized timeouts for different services
3. **Error Handling**: Graceful degradation when services are unavailable
4. **Caching**: Consider adding Redis for session storage

## üìû Support

If you encounter issues:
1. Check Render logs first
2. Verify environment variables
3. Test Ollama connectivity
4. Check health endpoint status

---

**Happy Deploying! üéâ**

